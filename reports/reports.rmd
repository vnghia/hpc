---
classoption: a4paper, xcolor = usenames,dvipsnames
geometry: margin=2.5cm
output:
  bookdown::pdf_document2:
    papersize: a4
    fig_caption: true
    highlight: tango
    keep_tex: false
    number_sections: true
    pandoc_args: 
      - "--listings"
    toc_depth: 3
    toc: false
    latex_engine: lualatex
    includes:
      in_header: preamble.tex
      before_body: cover.tex
---

```{r, chunk, include=F, cache=F}
knitr::read_chunk("reports.R")
```

```{r init, include=F}
```

# OpenMP
## Optimization techniques {#techniques}
### Naive dot

We first mention here the original `naive_dot` function. This function serves as an anchor (or base case) for performance comparision as well as for making sure we have the right result when using other techniques.

```{c, naive-dot-c, code=read_from_commit(blas3_source_code, 57:59, "7bc4b82"), eval=F}
```
```{r, naive-dot-shape, include=F}
```
Below is the output of `naive_dot` for `r replace_inline_code("M", m_naive_dot)`, `r replace_inline_code("K", k_naive_dot)` and `r replace_inline_code("N", n_naive_dot)`:
```{r, naive-dot-output, echo=F}
```

As
$$
```{r, naive-dot-true, results="asis", echo=F}
```
$$
The result of this function is correct. We move on to the next technique.

### Spatial locality

Spatial locality refers to the following scenario: if a particular storage location is referenced at a particular time, then it is likely that nearby memory locations will be referenced in the near future. In order to take advantages of this property, we notice that:

  - In memory, `A`, `B`, `C` are stored in contiguous memory block.
  - When using the index order `i`, `j`, `k`, we access `B` consecutively (as we access `B` by `B[k + ldb * j]`), but not `A` and `C`.
  - Data from `A`, `B`, `C` are loaded in a memory block consisting of severals consecutive elements to cache. Thus, we could make use of spatial locality when reading data continously.

From 3 points above, we decide to switch the index order to `k`, `j`, `i`. Now we see that both reading and writing operations on `C` are in cache, this brings us a critical gain in performance. In addition, reading operations on `A` are in cache too but those on `B` are not.

```{c, saxpy-dot-c, code=read_from_commit(blas3_source_code, 92:94, "cd150fe"), eval=F}
```
```{r, naive-saxpy-small-shape, include=F}
```
For comparision, we have a table below with `r replace_inline_code("M", m_naive_saxpy_small)`, `r replace_inline_code("K", k_naive_saxpy_small)` and `r replace_inline_code("N", n_naive_saxpy_small)`.
```{r, naive-saxpy-small-output, echo=F}
```
We have the frobenius norm of both techniques are `r to_float_str(naive_saxpy_small_df$norm[1])` which indicate we have the right computation result. In addition, calculating time is already significantly small ($\approx$ 0 second in both methods) and the difference between these two can therefore be ommited.
```{r, naive-saxpy-big-shape, include=F}
```
However, if we set `r replace_inline_code("M", default_m)`, `r replace_inline_code("K", default_k)` and `r replace_inline_code("N", default_n)`, there will be a huge performance gain as in the table shown below. In addition, from now, for an easier comparision between results, we will consider the default value of `M`, `K` and `N` is `r replace_inline_code("M", default_m)`, `r replace_inline_code("K", default_k)` and `r replace_inline_code("N", default_n)` if not explicitly mentioned.
```{r, naive-saxpy-big-output, echo=F}
```
Here, the `naive_dot` function is approximately `r to_float_str(round(naive_saxpy_big_df$time[1]/naive_saxpy_big_df$time[2], 2))` times slower than the `saxpy_dot` function.

### OpenMP parallelization

In this section, we will analyse the main technique of this chapter: `OpenMP`. First, we show how we enable it on each function. We add a directive above each loop we want to parallelize whose general form is as below:

- Variables inside `private` are tied to one specific thread (each thread has their own copies of those variables).
- `SCHEDULE_HPC` is replaced by the schedule^[https://www.openmp.org/spec-html/5.0/openmpse49.html#x288-20520006.1] we want.
- `NUM_THREADS_HPC` is corresponding to the number of threads to use for `parallel` regions.
```{c, init-omp, code=read_from_commit(blas3_source_code, 36:37), eval=F}
```

In addition, inside `norm` function, we add a reduction clause `reduction(+ : norm)` as we want to sum up every `norm` from each thread to one final `norm` and taking square of that final sum. Finally, we have to add `#pragma omp atomic` above each line that updating the result matrix (`C`). It is because that matrix is shared among threads, `atomic` makes sure that there is only one `+=` operation (which is essentially reading and writing) on one specific pair of indices at a given time. Note that `norm` does not need atomicity thank to `reduction`.
```{r, naive-saxpy-omp-shape, include=F}
```
Here, we show a comparision between with and without `OpenMP`. Default `OpenMP` options will be `r replace_inline_code("SCHEDULE_HPC", default_schedule[[1]]$name)` and `r replace_inline_code("NUM_THREADS_HPC", default_num_threads)`.
```{r, naive-saxpy-omp-output, echo=F}
```
Thank to `OpenMP`, naive approach is faster than `r to_float_str(round(naive_saxpy_omp_df$time[1]/naive_saxpy_omp_df$time[2], 2))` times while the `saxpy_dot` took less `r to_float_str(round(naive_saxpy_omp_df$time[3]/naive_saxpy_omp_df$time[4], 2))` times than before. Both approachs performance are significantly improved.

### Cache blocking (Tiled) {#cache-blocking}

The main idea of the cache blocking technique (or tiled) is breaking the whole matrices into smaller sub-matrices so the data needed for one multiplication operation could fit into the cache, therefore leads to a much faster calculation. Furthermore, if we enable `OpenMP`, the computation would be even faster as each sub-matrice is processed by a separate thread. However, if we set `BLOCK` size too small, the benefit of dividing matrix is overshadowed by the additional loops and operations. Meanwhile, a too large `BLOCK` size leads to an overfitting (data for one operation can not be fitted into the cache), and therefore a slower operation. The principal source code is shown below:
```{c, saxpy-dot-c, code=read_from_commit(blas3_source_code, 111:118, "ebcb8dc"), eval=F}
```
The above code will work only if `M`, `N` and `K` are divisible by `BLOCK`. A more generic version could be found in full source-code.

```{r,naive-saxpy-tiled-shape, include=F}
```
We have a table comparision between all techniques we are dicussing so far below. Also, we set the default size of `r replace_inline_code("BLOCK", default_block)`.
```{r, naive-saxpy-tiled-output, echo=F}
```
In the table above, cache blocking technique is already fast enough. However, `OpenMP` does not help speeding it up as the default `BLOCK` size is not optimized in this case.

### BLAS
One last technique that is used in our code is calling the `cblas_dgemm` function which use the optimized `BLAS` implementation. This function is the fastest method even if other methods are *"cheated"* (by using `OpenMP`)  as their implementation is optimized based on many factors: algorithms, software and hardware.
```{r, all-default-output, echo=F}
```

## Benchmarks
### Sequential
```{r, sequential-shape, include=F}
```

In this section, we fix `r replace_inline_code("NUM_THREADS_HPC", sequential_num_threads)` and vary the matrix size. Instead of using environmental variables, we use a script for generating code with the hard-coded configurations we want as reading environmental variables is an expensive operation.

For the sake of simplicity, we first consider the case where M and K and N are all equal and equal to a $2^s$ where `r replace_inline_code("s", ms_sequential[1])` to `r replace_inline_code("s", tail(ms_sequential, 1))`. In addition, we have included a non `OpenMP` result (which is also sequential) for studying how the overhead time of `OpenMP` impacts the overall performance.
```{r, sequential-output, echo=F}
```
```{r, sequential-plot, echo=F, fig.cap="Computational time in function of matrix size"}
sequential_plot
```

```{r, sequential-last-shape-output, echo=F}
```

In the graph below, we see that the fastest method is no doubt `blas` method, followed by `tiled`, `saxpy` and the slowest is `naive`. This is aligned with what we see in the [section 1](#techniques). In addition, the time for calculating matrices whose size is less than $2^{10} = 1024$ is around $5s$ for all methods. This could be explained by the fact that these matrices could be fitted entierly into the cache, which leads to a significant drop in computation time.

Another property that could be interesting is the version with `OpenMP` is close or even faster than the non `OpenMP` version regardless the overhead of parallelization. This could be explained by many factors ^[https://stackoverflow.com/questions/22927973/openmp-with-single-thread-versus-without-openmp] ^[https://stackoverflow.com/questions/2915390/openmp-num-threads1-executes-faster-than-no-openmp], but the most significant one is As `OpenMP` is just API specification and C compilers are free to implement it in any way they want as long as they respect the specification, many compilers (notably modern `gcc` and `clang`) are smart enough to treat `OpenMP` version of only 1 thread the same as the sequential version. Therefore, we only see a small difference between each run. If we run both versions enough times, the difference in average time of each will be the smaller.

### Threading
```{r, threading-shape, include=F}
```
Right now, we will be able to see the true power of parallelism, we will keep increasing the number of threads in form of $2^s$ where `r replace_inline_code("s", threading_num_threadss[1])` to `r replace_inline_code("s", tail(threading_num_threadss, 1))`.
```{r, threading-output, echo=F}
```
```{r, threading-plot, echo=F, fig.cap="Computational time in function of number of threads and schedule"}
threading_plot
```

We see that `BLAS` method is still the fastest regardless the number of threads and schedule (since it isn't affected by `OpenMP` options). It shows that in order to achieve high speed computation, we have to not only parallelize, but also make improvements on multiplication algorithms, memory accesses and even use assembly instructions.

In addition, the 4 schedule cuvres of each technique are overlapping each others and there are only very small difference in term of computational time. The phenomenon happened because our problem (matrix multiplication) has a nearly the same workload at each iterations. That means the first iteration will take almost the same as the last iteration or any other iterations. For each schedule:

- `static` evenly-divides the total workloads into each threads, which is the best schedule for our problem.
-  `dynamic` and `guided` are designed for different situation, where each iteration takes different amount of time to finish their work. There is overhead compared to `static`, however, it does not have big effect on overall performance as our matrices are not too big.
- `auto` lets the compiler choose how to schedule and divide work among threads, so it is compiler-specific. For example, `gcc` maps `auto` to `static` ^[https://github.com/gcc-mirror/gcc/blob/61e53698a08dc1d9a54d785218af687a6751c1b3/libgomp/loop.c#L195-L198], at a consequence, we see a similar pattern with `static`.

Finally, more threads **doesn't** always mean better performance. After we increased thread to 4, time taking for one multiplication fluctuates but does not have any real decline. The reason is there is only a limit number of physical cores inside each computer, when the number of threads goes up too high, the overhead in creating and synchronize threads will overshadowed any benefits we gain.

### Blocking
```{r, blocking-shape, include=F}
```
In the last section, we will concentrate ourselves on the impact of `BLOCK` size to overall performance. We will vary the `BLOCK` size in a power of 2, from `r replace_inline_code("s", blocking_blocks[1])` to `r replace_inline_code("s", tail(blocking_blocks, 1))`
```{r, blocking-output, echo=F}
```
```{r, blocking-plot, echo=F, fig.cap="Computational time in function of BLOCK size"}
blocking_plot
```

We see clearly that as `BLOCK` size grows, the performance generally becomes better but get worse after `BLOCK` size grows to approximately $2^{10}$. As explained in the [section 1](#cache-blocking), `BLOCK` should not be too small and neither too large.