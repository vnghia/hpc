---
documentclass: rapport3
classoption: xcolor = usenames,dvipsnames
output:
  bookdown::pdf_document2:
    papersize: a4
    fig_caption: true
    highlight: tango
    keep_tex: false
    number_sections: true
    pandoc_args: 
      - "--listings"
      - "--top-level-division=chapter"
    toc_depth: 3
    toc: false
    latex_engine: lualatex
    includes:
      in_header: preamble.tex
      before_body: cover.tex
---

```{r, chunk, include=F, cache=F}
knitr::read_chunk("reports.R")
```

```{r init, include=F}
```

# OpenMP
## Optimization techniques {#techniques}
### Naive dot

We first mention here the original `naive_dot` function. This function serves as an anchor (or base case) for performance comparision as well as for making sure we have the right result when using other techniques.

```{c, naive-dot-c, code=read_from_commit(blas3_source_code, 57:59, "7bc4b82"), eval=F}
```
```{r, naive-dot-shape, include=F}
```
Below is the output of `naive_dot` for `r replace_inline_code("M", m_naive_dot)`, `r replace_inline_code("K", k_naive_dot)` and `r replace_inline_code("N", n_naive_dot)`:
```{r, naive-dot-output, echo=F}
```

As
$$
```{r, naive-dot-true, results="asis", echo=F}
```
$$
The result of this function is correct. We move on to the next technique.

### Spatial locality

Spatial locality refers to the following scenario: if a particular storage location is referenced at a particular time, then it is likely that nearby memory locations will be referenced in the near future. In order to take advantages of this property, we notice that:

  - In memory, `A`, `B`, `C` are stored in contiguous memory block.
  - When using the index order `i`, `j`, `k`, we access `B` consecutively (as we access `B` by `B[k + ldb * j]`), but not `A` and `C`.
  - Data from `A`, `B`, `C` are loaded in a memory block consisting of severals consecutive elements to cache. Thus, we could make use of spatial locality when reading data continously.

From 3 points above, we decide to switch the index order to `k`, `j`, `i`. Now we see that both reading and writing operations on `C` are in cache, this brings us a critical gain in performance. In addition, reading operations on `A` are in cache too but those on `B` are not.

```{c, saxpy-dot-c, code=read_from_commit(blas3_source_code, 92:94, "cd150fe"), eval=F}
```
```{r, naive-saxpy-small-shape, include=F}
```
For comparision, we have a table below with `r replace_inline_code("M", m_naive_saxpy_small)`, `r replace_inline_code("K", k_naive_saxpy_small)` and `r replace_inline_code("N", n_naive_saxpy_small)`.
```{r, naive-saxpy-small-output, echo=F}
```
We have the frobenius norm of both techniques are `r to_float_str(naive_saxpy_small_df$norm[1])` which indicate we have the right computation result. In addition, calculating time is already significantly small ($\approx$ 0 second in both methods) and the difference between these two can therefore be ommited.
```{r, naive-saxpy-big-shape, include=F}
```
However, if we set `r replace_inline_code("M", default_m)`, `r replace_inline_code("K", default_k)` and `r replace_inline_code("N", default_n)`, there will be a huge performance gain as in the table shown below. In addition, from now, for an easier comparision between results, we will consider the default value of `M`, `K` and `N` is `r replace_inline_code("M", default_m)`, `r replace_inline_code("K", default_k)` and `r replace_inline_code("N", default_n)` if not explicitly mentioned.
```{r, naive-saxpy-big-output, echo=F}
```
Here, the `naive_dot` function is approximately `r to_float_str(round(naive_saxpy_big_df$time[1]/naive_saxpy_big_df$time[2], 2))` times slower than the `saxpy_dot` function.

### OpenMP parallelization

In this section, we will analyse the main technique of this chapter: `OpenMP`. First, we show how we enable it on each function. We add a directive above each loop we want to parallelize whose general form is as below:

- Variables inside `private` are tied to one specific thread (each thread has their own copies of those variables).
- `SCHEDULE_HPC` is replaced by the schedule^[https://www.openmp.org/spec-html/5.0/openmpse49.html#x288-20520006.1] we want.
- `NUM_THREADS_HPC` is corresponding to the number of threads to use for `parallel` regions.
```{c, init-omp, code=read_from_commit(blas3_source_code, 36:37), eval=F}
```

In addition, inside `norm` function, we add a reduction clause `reduction(+ : norm)` as we want to sum up every `norm` from each thread to one final `norm` and taking square of that final sum. Finally, we have to add `#pragma omp atomic` above each line that updating the result matrix (`C`). It is because that matrix is shared among threads, `atomic` makes sure that there is only one `+=` operation (which is essentially reading and writing) on one specific pair of indices at a given time. Note that `norm` does not need atomicity thank to `reduction`.
```{r, naive-saxpy-omp-shape, include=F}
```
Here, we show a comparision between with and without `OpenMP`. Default `OpenMP` options will be `r replace_inline_code("SCHEDULE_HPC", default_schedule[[1]]$name)` and `r replace_inline_code("NUM_THREADS_HPC", default_num_threads)`.
```{r, naive-saxpy-omp-output, echo=F}
```
Thank to `OpenMP`, naive approach is faster than `r to_float_str(round(naive_saxpy_omp_df$time[1]/naive_saxpy_omp_df$time[2], 2))` times while the `saxpy_dot` took less `r to_float_str(round(naive_saxpy_omp_df$time[3]/naive_saxpy_omp_df$time[4], 2))` times than before. Both approachs performance are significantly improved.
