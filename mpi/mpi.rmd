---
classoption: a4paper, xcolor = usenames,dvipsnames
geometry: margin=2.5cm
output:
  bookdown::pdf_document2:
    papersize: a4
    fig_caption: true
    highlight: tango
    keep_tex: false
    number_sections: true
    pandoc_args:
      - "--listings"
    toc_depth: 3
    toc: false
    latex_engine: lualatex
    includes:
      in_header: ../reports/preamble.tex
      before_body: ../reports/cover.tex
---
```{r, chunk, include=F, cache=F}
knitr::read_chunk(here::here("mpi", "mpi.r"))
```

```{r, init, include=F}
```

# MPI

In this section, we will focus on the optimization and parallelization of Google PageRank problem, more precisely, the calculation of the eigenvectors associated with the highest eigenvalue of a stochastic matrix that has some special attributes.

## Introduction

First of all, we mention briefly the problem. Given a web page $P$ and a measure of its importance $I(P)$,
called the page's PageRank. Suppose that page $P_{i}$ has $\ell_{i}$ links. If one of those links is to page $P_{j}$, then $P_{i}$ will pass on $frac{1}{\ell_{i}}$ of its importance to $P_{j}$. The importance ranking of $P_{j}$ is then the sum of all the contributions made by pages linking to it. That is, if we denote the set of pages linking to $P_{j}$ by $B_{j}$,
$$
I(P_{j}) = {\sum_{P_{i} \in B_{j}}\frac{I(P_{i})}{\ell_{i}}}
$$

Mathematically, if we define a matrix $H$ where:
$$
H_{i,j} =
\begin{cases}
\frac{1}{\ell_{i}} & \text{ if } P_{i} \in B_{j}\\
0 & \text{ otherwise }
\end{cases}
$$

$H$ is called "hyperlink matrix" and we have $\pi^{\intercal} = \pi^{\intercal} H$ where $\pi_i = I(P_{i})$, a vector contains measure of importance from all pages. In the other words, $(\pi^{\intercal})^{\intercal} = (\pi^{\intercal} H)^{\intercal} \implies \pi = H^{\intercal} \pi$ or $\pi$ is an eigenvector of $H^{\intercal}$ whose eigenvalue is $1$.

In addition, we could prove that the largest eigenvalue of $H^{\intercal}$ less than or equal to $1$. Indeed, if $\lambda$ is an eigenvalue of $H^{\intercal}$ and $u$ is its eigenvector (we will choose a vector whose the sum of all of its components is positive), as we can see all entries of $H$ are non negative and the sum of of each row is either $1$ or $0$ (if that page has no link to other page), we have:

$$
\begin{aligned}
& H^{\intercal} u = \lambda u \\
\iff &
\begin{pmatrix}
H_{1,1} & \dots & H_{n,1} \\
\vdots & \ddots & \vdots \\
H_{1,n} & \dots & H_{n,n} \\
\end{pmatrix}
\begin{pmatrix}
u_{1} \\
\vdots \\
u_{n} \\
\end{pmatrix}
=
\begin{pmatrix}
\lambda u_{1} \\
\vdots \\
\lambda u_{n} \\
\end{pmatrix} \\
\implies &
\begin{cases}
H_{1,1} u_{1} + \dots + H_{n,1} u_{n} &= \lambda u_{1} \: (1) \\
&\vdots \\
H_{1,n} u_{1} + \dots + H_{1,n} u_{n} &= \lambda u_{n} \: (n) \\
\end{cases}\\
& \text{ By summing up from } (1) \text{ to } (n) \, \\
\implies &
u_{1} (\sum_{i=1}^n H_{1,i}) + \dots + u_{n} (\sum_{i=1}^n H_{n,i}) = \lambda (\sum_{i=1}^n u_{i}) \\
& \text{As } \sum_{i=1}^n H_{j,i} = \text{ either } 0 \text{ or } 1 \: \forall j \\
\implies & \sum_{i=1}^n u_{i} \geq \lambda (\sum_{i=1}^n u_{i}) \\
\implies & \lambda \leq 1 \\
\end{aligned}
$$

Futhermore, if $\sum_{i=1}^n H_{j,i} = 1 \: \forall j$, we have $\lambda = 1$ is the largest eigenvalue of $H^{\intercal}$. Therefore, we return to the problem of finding the eigenvector associated with the largest eigenvalue of $H^{\intercal}$, which already has a very simple but powerful solution called "Power Iteration" that we will use in next sections. Each component of that vector is a measure of importance of one page and the index of the largest element inside that vector will also be the index of the most meaningful page.

## Power Iteration Method

```{r, mpi-function-definition, include=F}
```
```{r, init-read-notebook, include=F}
```

First, we choose a matrix $H$ as following for testing:
$$
```{r, init-introduction-matrix, results="asis", echo=F}
```
$$

As proved about, since every sum of a row of $H$ is equal to $1$, the largest eigenvalue will be $1$ and therefore, we could apply this method to find out the PageRank score of each page.

The method could be described as follow:

```{=latex}
\begin{algorithm}[H]
  \KwData{A diagonalizable matrix $G$}
  \KwResult{Largest eigenvalue and associated eigenvector}
  $u \leftarrow random$ \;
  $u \leftarrow \frac{u}{\left\lVert u \right\rVert}$ \;
  \While{$\left\Vert u-v\right\Vert >\epsilon$}{
    compute $v=Gu$ \;
    define $\lambda=u^{\intercal}v$ and $u=\frac{v}{\Vert v\Vert}$ \;
  }
\end{algorithm}
```

And its implementation in `Python`:
```{python, power-iteration-method-base, code=nb_sources[[4]][c(2:12,19,39:41,43)], eval=F}
```

```{r, power-iteration-introduction-matrix, include=F}
```

Here, we choose to keep the original power iteration method by calculating $v=Gu$ instead of $v=G^{\intercal}u$. It helps us prevent some confusions. Since we want to calculate the eigenvector of $H^{\intercal}$, we could pass $G \leftarrow H^{\intercal}$. From the output below, the method has successfully converged to $1$ and the most meaningful page is the page number `r power_introduction_result[[2]]$important`.

```{r, power-introduction-matrix-output, echo=F}
```

In addition, we check the correctness by calling the function `eigs` from `scipy.sparse.linalg`. As we can see, the two outputs are the similar. Our implementation of power iteration is correct.
```{python, power-introduction-matrix-correctness, code=nb_sources[[2]][c(1,3:4, 6:7)]}
```
