---
classoption: a4paper, xcolor = usenames,dvipsnames
geometry: margin=2.5cm
output:
  bookdown::pdf_document2:
    papersize: a4
    fig_caption: true
    highlight: tango
    keep_tex: false
    number_sections: true
    pandoc_args:
      - "--listings"
    toc_depth: 3
    toc: false
    latex_engine: lualatex
    includes:
      in_header: ../reports/preamble.tex
      before_body: ../reports/cover.tex
---
```{r, chunk, include=F, cache=F}
knitr::read_chunk(here::here("mpi", "mpi.r"))
```

```{r, init, include=F}
```

# MPI

In this section, we will focus on the optimization and parallelization of Google PageRank problem, more precisely, the calculation of the eigenvectors associated with the highest eigenvalue of a stochastic matrix that has some special attributes.

## Introduction

First of all, we mention briefly the problem. Given a web page $P$ and a measure of its importance $I(P)$,
called the page's PageRank. Suppose that page $P_{i}$ has $\ell_{i}$ links. If one of those links is to page $P_{j}$, then $P_{i}$ will pass on $frac{1}{\ell_{i}}$ of its importance to $P_{j}$. The importance ranking of $P_{j}$ is then the sum of all the contributions made by pages linking to it. That is, if we denote the set of pages linking to $P_{j}$ by $B_{j}$,
$$
I(P_{j}) = {\sum_{P_{i} \in B_{j}}\frac{I(P_{i})}{\ell_{i}}}
$$

Mathematically, if we define a matrix $H$ where:
$$
H_{i,j} =
\begin{cases}
\frac{1}{\ell_{i}} & \text{ if } P_{i} \in B_{j}\\
0 & \text{ otherwise }
\end{cases}
$$

$H$ is called "hyperlink matrix" and we have $\pi^{\intercal} = \pi^{\intercal} H$ where $\pi_i = I(P_{i})$, a vector contains measure of importance from all pages. In the other words, $(\pi^{\intercal})^{\intercal} = (\pi^{\intercal} H)^{\intercal} \implies \pi = H^{\intercal} \pi$ or $\pi$ is an eigenvector of $H^{\intercal}$ whose eigenvalue is $1$.

In addition, we could prove that the largest eigenvalue of $H^{\intercal}$ less than or equal to $1$. Indeed, if $\lambda$ is an eigenvalue of $H^{\intercal}$ and $u$ is its eigenvector (we will choose a vector whose the sum of all of its components is positive), as we can see all entries of $H$ are non negative and the sum of of each row is either $1$ or $0$ (if that page has no link to other page), we have:

$$
\begin{aligned}
& H^{\intercal} u = \lambda u \\
\iff &
\begin{pmatrix}
H_{1,1} & \dots & H_{n,1} \\
\vdots & \ddots & \vdots \\
H_{1,n} & \dots & H_{n,n} \\
\end{pmatrix}
\begin{pmatrix}
u_{1} \\
\vdots \\
u_{n} \\
\end{pmatrix}
=
\begin{pmatrix}
\lambda u_{1} \\
\vdots \\
\lambda u_{n} \\
\end{pmatrix} \\
\implies &
\begin{cases}
H_{1,1} u_{1} + \dots + H_{n,1} u_{n} &= \lambda u_{1} \: (1) \\
&\vdots \\
H_{1,n} u_{1} + \dots + H_{1,n} u_{n} &= \lambda u_{n} \: (n) \\
\end{cases}\\
& \text{ By summing up from } (1) \text{ to } (n) \, \\
\implies &
u_{1} (\sum_{i=1}^n H_{1,i}) + \dots + u_{n} (\sum_{i=1}^n H_{n,i}) = \lambda (\sum_{i=1}^n u_{i}) \\
& \text{As } \sum_{i=1}^n H_{j,i} = \text{ either } 0 \text{ or } 1 \: \forall j \\
\implies & \sum_{i=1}^n u_{i} \geq \lambda (\sum_{i=1}^n u_{i}) \\
\implies & \lambda \leq 1 \\
\end{aligned}
$$

Futhermore, if $\sum_{i=1}^n H_{j,i} = 1 \: \forall j$, we have $\lambda = 1$ is the largest eigenvalue of $H^{\intercal}$. Therefore, we return to the problem of finding the eigenvector associated with the largest eigenvalue of $H^{\intercal}$, which already has a very simple but powerful solution called "Power Iteration" that we will use in next sections. Each component of that vector is a measure of importance of one page and the index of the largest element inside that vector will also be the index of the most meaningful page.

## Power Iteration Method

```{r, mpi-function-definition, include=F}
```
```{r, init-read-notebook, include=F}
```

First, we choose a matrix $H$ as following for testing:
$$
```{r, init-introduction-matrix, results="asis", echo=F}
```
$$

As proved about, since every sum of a row of $H$ is equal to $1$, the largest eigenvalue will be $1$ and therefore, we could apply this method to find out the PageRank score of each page.

The method could be described as follow:

```{=latex}
\begin{algorithm}[H]
  \KwData{A diagonalizable matrix $G$}
  \KwResult{Largest eigenvalue and associated eigenvector}
  $u \leftarrow random$ \;
  $u \leftarrow \frac{u}{\left\lVert u \right\rVert}$ \;
  \While{$\left\Vert u-v\right\Vert >\epsilon$}{
    compute $v=Gu$ \;
    define $\lambda=u^{\intercal}v$ and $u=\frac{v}{\Vert v\Vert}$ \;
  }
\end{algorithm}
```

And its implementation in `Python`:
```{python, power-iteration-method-base, code=nb_sources[[4]][c(2:12,19,39:41,43)], eval=F}
```

```{r, power-iteration-introduction-matrix, include=F}
```

Here, we choose to keep the original power iteration method by calculating $v=Gu$ instead of $v=G^{\intercal}u$. It helps us prevent some confusions. Since we want to calculate the eigenvector of $H^{\intercal}$, we could pass $G \leftarrow H^{\intercal}$. From the output below, the method has successfully converged to $1$ and the most meaningful page is the page number `r power_introduction_result[[2]]$important`.

```{r, power-introduction-matrix-output, echo=F}
```

In addition, we check the correctness by calling the function `eigs` from `scipy.sparse.linalg`. As we can see, the two outputs are the similar. Our implementation of power iteration is correct.
```{python, power-introduction-matrix-correctness, code=nb_sources[[2]][c(1,3:4, 6:7)]}
```

## Google matrix
```{r, google-matrix, include=F}
```

In general, there is no guarantee that the algorithm will work. As we can see in the introduction, if one web page does not link to any other page or also called as a dangling node, the sum of that row in the hyperlink matrix will be $0$, the largest eigenvalue will be eventually $<$ 1 and therefore we fail. To address this problem, we introduce another type of matrix, so-called Google matrix and use this matrix for calculating the PageRank. It has the following property:
$$
\begin{aligned}
G &= \alpha(H + \frac{1}{n} d e^{\intercal}) + (1 - \alpha)\frac{1}{n}e e^{\intercal} \\
&\text{Where} \\
H&: \text{ a very sparse hyperlink matrix.} \\
\alpha&: \text{ a scaling parameter between 0 and 1 (generally set at the “magic” value of $0.85$).} \\
d&: \text{ the binary dangling node row vector ($d_{i}$ = 1 if page $i$ is a dangling node and 0 otherwise).} \\
e^{\intercal}&: \text{ the row vector of all entries 1.} \\
\end{aligned}
$$

We could easily prove that $G$ is stochastic, i.e, sum of each row equals to $1$. Indeed,
$$
\begin{aligned}
\sum_{i=1}^n G_{i,j} &= \alpha (\sum_{i=1}^n H_{i,j} + \frac{1}{n} \sum_{1}^n d_{j}) + (1 - \alpha) \\
\implies \sum_{i=1}^n G_{i,j} &= \alpha ((\sum_{i=1}^n H_{i,j}) + d_{j}) + 1 - \alpha \\
\text{Since one of } \sum_{i=1}^n H_{i,j} \text{ or } d_{j} &\text{ equals to 1 and the other term equals to zero } \\
\implies \sum_{i=1}^n G_{i,j} &= \alpha + 1 - \alpha \\
\implies \sum_{i=1}^n G_{i,j} &= 1
\end{aligned}
$$

We have two approaches for this problem.

### Dense

In this approach, we compute in the "traditional" way, i.e, we construct the matrix $G$ and pass it to the power iteration method. The output is shown below.
```{r, google-dense, echo=F}
```

### Sparse

On the other hand, we could take advantage of two factors: the sparsity of $H$ and the nature of the matrices $de^{\intercal}$ and $ee^{\intercal}$. First, we note that we want to calculating the eigenvector of 
$$G^{\intercal} = \alpha(H^{\intercal} + \frac{1}{n} e d^{\intercal}) + (1 - \alpha)\frac{1}{n}e e^{\intercal}$$
The matrix multiplication between $G^{\intercal}$ and $x$ turns out to be 
$$G^{\intercal}x = \alpha H^{\intercal}x + \alpha \frac{1}{n} e d^{\intercal} x + (1 - \alpha)\frac{1}{n}e e^{\intercal} x$$
Futhermore, give $a$ an arbitrary column vector,
$$
\begin{aligned}
e a^{\intercal} x &=
\begin{pmatrix}
1 \\
\vdots \\
1 \\
\end{pmatrix}
\begin{pmatrix}
a_{1} & \dots & a_{n} \\
\end{pmatrix}
\begin{pmatrix}
x_{1} \\
\vdots \\
x_{n} \\
\end{pmatrix} \\
&=
\begin{pmatrix}
a_{1} & \dots & a_{n} \\
\vdots & \ddots & \vdots \\
a_{1} & \dots & a_{n} \\
\end{pmatrix}
\begin{pmatrix}
x_{1} \\
\vdots \\
x_{n} \\
\end{pmatrix} \\
&=
\begin{pmatrix}
a_{1}x_{1} + \dots + a_{n}x_{n} \\
\vdots \\
a_{1}x_{1} + \dots + a_{n}x_{n}  \\
\end{pmatrix} \\
&=
\langle a, x \rangle
\begin{pmatrix}
1 \\
\vdots \\
1  \\
\end{pmatrix}
\end{aligned}
$$
Therefore, instead of calculating $e d^{\intercal} x$ and $e e^{\intercal}x$, we could just calculate two scalars $\langle d, x \rangle$ and $\langle e, x \rangle$. Thank to the fact that in `Python` world, an operation between an array and a scalar will be resulted in an element-wise operation, we have our desired matrix multiplication. Its implementation in `Python` is:
```{python, matvec-sparse-implement, code=nb_sources[[7]][c(2:9)], eval=F}
```

The output of this method is shown below.
```{r, google-sparse, echo=F}
```

### Benchmarking
```{r, google-dense-sparse, include=F}
```

We see the difference between two methods in term of performance is quite small, given that our matrix is too tiny. We varied the initial matrix $H$ and observeb that, while the sparse approach performs worse in small cases, it gets better when the size of $H$ increases. That could be explained by the fact that the density of $H$ (number of non-zero elements) decreases when the size of $H$ is bigger (based on the external data). Since $0$ is a really special number, a sparsity-aware matrix could take advantage of that fact and perform a lot faster. In our case, the speed-up comes from two factors:

- `M.dot(x)` as $M \leftarrow H$ is a sparse matrix, which means `M` contains only non-zero entries. Therefore while normal algorithm have to do operations with every elements of `M` and `x`, the sparse algorithm will only doing operations between those elements of `M` and `x` ($0$ multiplies something is $0$ anyway) which reduce significanly the number of operations required. if we convert $H$ to a normal matrix beforehand, the performance will be similar to the dense approach.
- When the size of $H$ is too big to be fitted within the RAM, we have an Out Of Memory (OOM) if we try the first approach. The last matrix `ucam2006`, the dense version needs roughly `r to_float_str(google_dense_sparse_df$memory[[9]])` $\si{\giga\byte}$ which is impossible on almost computers. On the opposite side, the second approach only needs a fraction of that amount of memory to store the matrix $H$ since its density is relatively low.

Below is a table that shown the results above where:

- density: the number of non-zero elements of $H$ per mille.
- shape: the number of rows of $H$ (and also of columns because $H$ is a square matrix).
- important: the most meaningful page, we show it here just for asserting the true result.
- memory: the amount of RAM memory in $\si{\giga\byte}$ that is needed to fit the dense version of that matrix.

```{r, google-dense-sparse-output, echo=F}
```

## MPI-enabled

Now, since, we has already taken advantage of the sparsity of matrix for optimizing the power iteration method, algorithm-level is now harder to optimize. However, we could try to fully exploit our hardware by parallelization, which will be easier. In this section, we concentrate ourselves in `MPI`, one technology that will allows us to achieve that.

### Principal functions
First, we will integrate `MPI` to 4 principal functions: $\| \cdot \|_1$, $\| \cdot \|_2$, $\langle \cdot,\cdot \rangle$ and `matvec` (which is essentially matrix multiplication with the second matrix is a vector).

For the first 3 functions, the principle is simple, we calculate them with a subset of values in each process and reduce them into one at the end. For example, $\langle \cdot,\cdot \rangle$ could be calculated as follow:
$$
\begin{aligned}
\langle x,y \rangle &= \sum_{i=1}^{kn} x_{i} y_{i} \\
&= \sum_{i=1}^{k}\sum_{j=(i-1)n + 1}^{ni} x_{j} y_{j} \\
&= \sum_{i=1}^{k} 
\underbrace{
\langle 
\begin{pmatrix}
x_{(i-1)n + 1} \\
\vdots \\
x_{in} \\
\end{pmatrix}
,
\begin{pmatrix}
y_{(i-1)n + 1} \\
\vdots \\
y_{in} \\
\end{pmatrix}
}_{\text{calculate in process } i}
\rangle
\end{aligned}
$$
Our implementation of those 3 functions with `MPI` is shown below. Note that `u` and `v` is only a subset of the real vectors `u` and `v`. However, since we reduce the result of each process in the end and send it to all processes by `Allreduce`, we still get the true value of these operations on vectors `u` and `v` regardless process.
```{python, code=read_from_commit(here::here("mpi", "norms_mpi.py"), c(31:49), commit = "5adcd24"), eval=F}
```

For `matvec`, everything get more complicated, we note that, given a matrix $H$ and $u$, if we note $v = Hu$, we have:
$$
v_{i} = \sum_{j=1}^n H_{i,j} u_{j}
$$
Therefore, the vector that passed into `matvec` must be a "full" vector.


### Power Iteration

With all those principles in mind, we are ready now for a `MPI`-enabled power iteration. The general idea will be:
```{=latex}
\begin{algorithm}[H]
  \DontPrintSemicolon
  \KwData{A diagonalizable matrix $G$ whose size is $kn$ and $k$ processes}
  \KwResult{Largest eigenvalue and associated eigenvector}
  In process $i$\;
  \Indp
    $v_{i} \leftarrow $ random\;
    gather $u \leftarrow \begin{pmatrix} v_{1} & \dots & v_{k} \end{pmatrix}$ (each process has their own copy of $u$)\;
  \Indm
  Define\;
  \Indp
    $G_{i} = \begin{pmatrix} G_{(i - 1)n + 1, \mathlarger{\cdot}} & \dots & G_{in, \mathlarger{\cdot}} \end{pmatrix}^{\intercal}$\;
  \Indm
  \While{$\left\Vert u_{i} - v\right\Vert >\epsilon$}{
    define $G_{i}$ and $u_{i}$\;
    compute $v_{i}=G_{i}u$ or in other world $v_{i} = \begin{pmatrix}[Gu]_{(i - 1)n + 1} & \dots & [Gu]_{in}\end{pmatrix}^{\intercal}$\;
    by using MPI-enabled functions above\;
    \Indp
      gather $v \leftarrow \begin{pmatrix} v_{1} & \dots & v_{k} \end{pmatrix}$\;
      define $\lambda=u^{\intercal}v$ and $v_i=\frac{v_i}{\Vert v\Vert}$\;
    \Indm
    gather $u \leftarrow \begin{pmatrix} v_{1} & \dots & v_{k} \end{pmatrix}$\;
  }
\end{algorithm}
```

#### Dense {-}
For the dense approach, the main logic is implemented as follow:
```{python, code=read_from_commit(here::here("mpi", "pagerank_dense.py"), c(113,114,117:119), commit = "836ae8b"), eval=F}
```
Inside the code above, `matvec` is just a normal matrix multiplication. The step gather `v` above is executed inside `dot_product` and `norm2` functions. `mpi_all_to_all` is the function we used in order to gather `u` at the end with the help of `Allgather` or `Allgatherv` underlying.

#### Sparse {-}
However, the sparse method requires a slightly more complicated solution. It has one different line from the the first version.
```{python, code=read_from_commit(here::here("mpi", "pagerank_sparse.py"), c(114:118), commit = "fb36519"), eval=F}
```
Where `matvec` is
```{python, code=read_from_commit(here::here("mpi", "pagerank_sparse.py"), c(74:79,84:88), commit = "fb36519"), eval=F}
```
Although, `dot_product` produces a global result, it is still true because it is a scalar, not a vector as show above.

### Benchmarking
```{r, google-dense-sparse-mpi, include=F}
```
```{r, google-dense-sparse-mpi-plot, echo=F, fig.cap="mpi vs non-mpi"}
mpi_google_dense_sparse_plot
```
First, both versions (non `MPI` and `MPI`) using the dense algorithm is defeated by the matrix `ucam2006` as explained above. Second, interestingly, when number of processes is `r mpi_np_default`, it took more time than non `MPI` version. This could be explained by the fact that `MPI` is optimized for a distributed cluster of machines but we are testing on only one computer, hence, the communication overhead overshaddowed any benefit it brought to us.